# ğŸ” AI Research Hub ğŸ’¡

## Overview ğŸš€

The AI Research Hub is a web application designed to provide insightful answers and relevant research findings based on your queries. It harnesses the power of Large Language Models (LLMs) and sophisticated information retrieval techniques to delve into complex topics and present comprehensive results.

At its core, the backend features a **Deep Research AI Agentic System** ğŸ¤–ğŸ§  employing a dual-agent architecture orchestrated with **LangGraph** ğŸ”— and **LangChain** ğŸ”—ğŸ§© frameworks. This system is designed as follows:

* **Research & Data Collection Agent ğŸ•µï¸â€â™€ï¸:** This agent focuses on actively crawling websites and gathering online information using **Tavily** ğŸŒğŸ”.
* **Answer Drafting Agent âœï¸:** This agent takes the organized information collected by the research agent and synthesizes it into a comprehensive and well-structured answer ğŸ“.

The frontend (React âš›ï¸) provides a user-friendly interface to interact with this powerful research system.

### Frontend Interface ğŸ–¥ï¸

**Search page**
![Frontend Screenshot Placeholder](images/i1.png)

**Agent Answer**
![Backend Architecture Placeholder](images/i2.png)

**Sources used**
![Research Output Example Placeholder](images/i3.png)

## Technologies Used ğŸ› ï¸

* Python ğŸ
* FastAPI ğŸš€
* LangChain ğŸ”—ğŸ§©
* LangGraph ğŸ”—ğŸ•¸ï¸
* Tavily ğŸŒğŸ”
* Uvicorâš™ï¸
* React âš›ï¸
* CSS ğŸ’…
* JavaScript ğŸ§©

## Setup and Installation âš™ï¸

### Frontend âš›ï¸

1.  Navigate to the `frontend` directory in your project.
2.  Install dependencies:
    ```bash
    npm install
    # or
    yarn install
    ```
3.  Start the development server:
    ```bash
    npm run dev
    # or
    yarn dev
    ```
    The frontend will typically be accessible at `http://localhost:3000` in your browser ğŸŒ.

### Backend ğŸğŸ”—

1.  Navigate to the `backend` directory in your project.
2.  Create a virtual environment (recommended for a clean setup ğŸª´):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Linux/macOS ğŸ§ğŸ
    venv\Scripts\activate  # On Windows ğŸªŸ
    ```
3.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
4.  Navigate to the `deep_research_agent` directory.
5.  Run the Langchain/LangGraph research agent directly :
    ```bash
    python main.py
    ```
6.  To run the FastAPI backend API server ğŸš€:
    ```bash
    cd ../  
    uvicorn main:app --reload
    ```
    The backend API will typically be accessible at `http://127.0.0.1:8000` ğŸ‘‚.

**Important Note:** Make sure your backend API keys (for LLMs ğŸ”‘, Tavily ğŸ”‘, etc.) are properly configured as environment variables or within your backend code for the AI to work its magic âœ¨.

## Usage ğŸ§‘â€ğŸ’»

1.  Start both the frontend (âš›ï¸) and backend (ğŸğŸ”—) development servers.
2.  Open your favorite web browser ğŸŒ and go to the frontend URL (usually `http://localhost:3000`).
3.  Type your burning research question ğŸ¤” into the input field.
4.  Click the "Search" button ğŸ–±ï¸.
5.  The frontend will send your insightful query to the backend API ğŸ“¡.
6.  The backend's **dual-agent system** ğŸ¤–ğŸ¤–, powered by LangGraph ğŸ”—ğŸ•¸ï¸ and utilizing Tavily ğŸŒğŸ” for web crawling, will process your request. The research agent ğŸ•µï¸â€â™€ï¸ will gather information, and the answer drafting agent âœï¸ will synthesize the findings.
7.  The backend will send back a comprehensive drafted answer ğŸ“ and the relevant research sources ğŸ“š to the frontend.
8.  The frontend will proudly display the AI-powered results for your enlightenment ğŸ’¡.

## License ğŸ“œ

This project is licensed under the MIT License. ğŸ“